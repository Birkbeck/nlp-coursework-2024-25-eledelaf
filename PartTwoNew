# Part 2:Feature Extraction and Classification
"""
Read the hansard40000.csv dataset in the texts directory into a dataframe. 
Sub-set and rename the dataframe as follows:
"""
import pandas as pd

df = pd.read_csv("/Users/elenadelafuente/Desktop/MASTER/2 trimestre/Natural Lenguage Processing/Assesment/p2-texts/hansard40000.csv")

"""
Rename the ‘Labour (Co-op)’ value in ‘party’ column to ‘Labour’
"""
df = df.replace({"Labour (Co-op)": "Labour"})

"""
remove any rows where the value of the ‘party’ column is not one of the
four most common party names, and remove the ‘Speaker’ value.
"""
df = df[df["party"]!= "Speaker"]
parties_4 = list(df['party'].value_counts()[:4].index)
df = df[df['party'].isin(parties_4)]

"""
remove any rows where the value in the ‘speech_class’ column is not
‘Speech’.
"""
df = df[df['speech_class'] == 'Speech']

"""
remove any rows where the text in the ‘speech’ column is less than 1000
characters long.
"""
final_df = df[df["speech"].str.len() >= 1000]
"""
Implement a new custom tokenizer and pass it to the tokenizer argument of
Tfidfvectorizer. 
"""
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import f1_score
from sklearn.metrics import classification_report

import spacy 
nlp = spacy.load("en_core_web_sm")

def custom_tokenizer(text):
    doc = nlp(text)
    tokens = []
    for token in doc:
        # We skip the stopword, punctuation, or numbers
        if token.is_stop:
            continue
        if token.is_punct:
            continue
        if token.like_num:
            continue

        # We make a lemmatization 
        new_token = token.lemma_
        tokens.append(new_token.lower().strip())
    return tokens
 
"""
You can use this function in any way you like to try to achieve
the best classification performance while keeping the number of features to no
more than 3000, and using the same three classifiers as above. 
"""
vectorizer =  TfidfVectorizer(tokenizer = custom_tokenizer, max_features = 3000)
X = vectorizer.fit_transform(final_df["speech"])
y = final_df["party"]

# Print the classification report as in 2(c) again using these parameters.
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=26, stratify=y)
# Train RandomForest (with n_estimators=300)
r_Forest = RandomForestClassifier(n_estimators=300, class_weight='balanced')
r_Forest.fit(X_train, y_train)
y_rF_predict = r_Forest.predict(X_test)

# Train the SVM with linear kernel classifiers
svm = SVC(kernel= 'linear')
svm.fit(X_train, y_train)
y_svm_predict = svm.predict(X_test)

# Print the classification
print("Classification report Random Forest(3)")
# Aqui hay un error
print(classification_report(y_test,y_rF_predict))  

print("Classification report svm(3)")
print(classification_report(y_test,y_svm_predict))

"""
Print the classification report for the best performing classifier using your tokenizer. Marks
will be awarded both for a high overall classification performance, and a good
trade-off between classification performance and eﬀiciency (i.e., using fewer pa-
rameters).
"""
